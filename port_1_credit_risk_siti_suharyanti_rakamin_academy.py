# -*- coding: utf-8 -*-
"""Port 1 - Credit Risk -  Siti Suharyanti - Rakamin Academy.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1S7bFBdn6tTg1vfcThcuA-atQXHjaYGSW

##**IMPORT LIBRARY**
"""

import numpy as np
import pandas as pd
pd.set_option('display.max_columns', None) # Atur jumlah max kolom yang ditampilkan = tidak terbatas
pd.set_option('display.max_rows', 99) # Atur jumlah max baris yang ditampilkan = 99
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split, GridSearchCV
# from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
from imblearn.over_sampling import SMOTE
from sklearn.ensemble import RandomForestClassifier
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import roc_curve, roc_auc_score

"""##**IMPORT DATA**"""

!gdown --id 1DfTQHlw9xWb_T0iRDnumm7PH_jEdA2He

# Membaca file csv
# index_col=0 menggunakan nilai di kolom pertama sebagai indeks DataFrame
data = pd.read_csv('loan_data_2007_2014.csv', index_col=0)

"""##**EXPLORE DATA**"""

# Mengetahui jumlah baris dan kolom
data.shape

# Menampilkan informasi tentang struktur dan karakteristik dataset
data.info()

# Menampilkan satu baris secara acak
data.sample()

# Menghitung jumlah nilai unik pada id
data.id.nunique()

# Menghitung jumlah nilai unik pada member_id
data.member_id.nunique()

"""###Remove Useless Features"""

cols_to_drop = [
    # unique id
    'id'
    , 'member_id'

    # free text
    , 'url'
    , 'desc'

    # all null / constant / others
    , 'zip_code'
    , 'annual_inc_joint'
    , 'dti_joint'
    , 'verification_status_joint'
    , 'open_acc_6m'
    , 'open_il_6m'
    , 'open_il_12m'
    , 'open_il_24m'
    , 'mths_since_rcnt_il'
    , 'total_bal_il'
    , 'il_util'
    , 'open_rv_12m'
    , 'open_rv_24m'
    , 'max_bal_bc'
    , 'all_util'
    , 'inq_fi'
    , 'total_cu_tl'
    , 'inq_last_12m'

    # expert judgment
    , 'sub_grade'
]

# axis = 1 --> menghapus kolom
# axis = 0 --> menghapus baris, default
new_data = data.drop(cols_to_drop, axis=1)

new_data.sample(5)

"""## **DEFINE TARGET VARIABLE / LABELING**"""

new_data.loan_status.value_counts()

# Normalisasi --> membandingkan proporsi masing-masing kategori dalam data secara lebih tepat, terlepas dari ukuran total data.
new_data.loan_status.value_counts(normalize=True)*100

bad_status = [
    'Charged Off'
    , 'Default'
    , 'Does not meet the credit policy. Status: Charged Off'
    , 'Late (31-120 days)'
]

new_data['status_loan'] = np.where(new_data['loan_status'].isin(bad_status), 1, 0)

new_data['status_loan'].value_counts()

new_data['status_loan'].value_counts(normalize=True)*100

# Hapus kolom loan_status
# Inplace=True --> DataFrame asli akan dimodifikasi secara langsung.
new_data.drop('loan_status', axis=1, inplace=True)

"""##**CLEANING, PREPROCESSING, FEATURE ENGINEERING**

###emp_length

Mengubah string menjadi float
"""

new_data['emp_length'].unique()

new_data['emp_length_int'] = new_data['emp_length'].str.replace(r'[^0-9]+', '', regex=True)

new_data['emp_length_int'] = new_data['emp_length_int'].astype(float)

new_data['emp_length_int']

new_data.drop('emp_length', axis=1, inplace=True)

"""###term

Mengubah string menjadi float
"""

new_data['term'].unique()

new_data['term_int'] = new_data['term'].str.replace(' months', '')

new_data['term_int'] = new_data['term_int'].astype(float)

new_data['term_int']

new_data.drop('term', axis=1, inplace=True)

"""###earliest_cr_line

Memodifikasi `earliest_cr_line` dari format bulan-tahun menjadi perhitungan berapa lama waktu berlalu sejak waktu tersebut.
"""

new_data['earliest_cr_line'].head()

new_data['earliest_cr_line_date'] = pd.to_datetime(new_data['earliest_cr_line'], format='%b-%y')

new_data['earliest_cr_line_date'].head()

new_data['mths_since_earliest_cr_line'] = round(pd.to_numeric((pd.to_datetime('2017-12-01') - new_data['earliest_cr_line_date']) / np.timedelta64(1, 'M')))

new_data['mths_since_earliest_cr_line'].head()

new_data['mths_since_earliest_cr_line'].describe()

new_data[new_data['mths_since_earliest_cr_line']<0][['earliest_cr_line', 'earliest_cr_line_date', 'mths_since_earliest_cr_line']].head()

"""Nilai negatif muncul karena fungsi Python salah menginterpretasikan tahun 62 menjadi tahun 2062, padahal seharusnya merupakan tahun 1962.

Cara mengatasinya degan mengubah nilai yang negatif menjadi nilai maximum dari fitur tersebut. Karena nilai-nilai yang negatif artinya adalah data yang sudah tua (tahun 1900an), maka masih masuk akal jika mengganti nilai-nilai tersebut menjadi nilai terbesar.
"""

#  loc = mengakses baris-baris yang memenuhi kondisi yang diberikan.
new_data.loc[new_data['mths_since_earliest_cr_line']<0, 'mths_since_earliest_cr_line'] = new_data['mths_since_earliest_cr_line'].max()

new_data.drop(['earliest_cr_line', 'earliest_cr_line_date'], axis=1, inplace=True)

"""###issue_d"""

new_data['issue_d_date'] = pd.to_datetime(new_data['issue_d'], format='%b-%y')
new_data['mths_since_issue_d'] = round(pd.to_numeric((pd.to_datetime('2017-12-01') - new_data['issue_d_date']) / np.timedelta64(1, 'M')))

new_data['mths_since_issue_d'].describe()

new_data.drop(['issue_d', 'issue_d_date'], axis=1, inplace=True)

"""###last_pymnt_d"""

new_data['last_pymnt_d_date'] = pd.to_datetime(new_data['last_pymnt_d'], format='%b-%y')
new_data['mths_since_last_pymnt_d'] = round(pd.to_numeric((pd.to_datetime('2017-12-01') - new_data['last_pymnt_d_date']) / np.timedelta64(1, 'M')))

new_data['mths_since_last_pymnt_d'].describe()

new_data.drop(['last_pymnt_d', 'last_pymnt_d_date'], axis=1, inplace=True)

"""### next_pymnt_d"""

new_data['next_pymnt_d_date'] = pd.to_datetime(new_data['next_pymnt_d'], format='%b-%y')
new_data['mths_since_next_pymnt_d'] = round(pd.to_numeric((pd.to_datetime('2017-12-01') - new_data['next_pymnt_d_date']) / np.timedelta64(1, 'M')))

new_data['mths_since_next_pymnt_d'].describe()

new_data.drop(['next_pymnt_d', 'next_pymnt_d_date'], axis=1, inplace=True)

"""### last_credit_pull_d"""

new_data['last_credit_pull_d_date'] = pd.to_datetime(new_data['last_credit_pull_d'], format='%b-%y')
new_data['mths_since_last_credit_pull_d'] = round(pd.to_numeric((pd.to_datetime('2017-12-01') - new_data['last_credit_pull_d_date']) / np.timedelta64(1, 'M')))

new_data['mths_since_last_credit_pull_d'].describe()

new_data.drop(['last_credit_pull_d', 'last_credit_pull_d_date'], axis=1, inplace=True)

"""##**EXPLORATORY DATA ANALYSIS**

### Correlation Check
"""

plt.figure(figsize=(20,15))
# annot=True --> Menampilkan nilai korelasi setiap sel
sns.heatmap(new_data.corr(), annot=True, fmt='.1f')

"""Nilai korelasi yang dijadikan patokan sebagai korelasi tinggi tidak pasti, umumnya digunakan angka 0.7.

Mengidentifikasi variabel yang mungkin saling terkait secara kuat dalam analisis data, yang dapat menyebabkan masalah multicollinearity dalam model statistik atau machine learning. **Variabel** dengan **korelasi tinggi** sering kali **tidak memberikan informasi tambahan** yang signifikan dan **dapat dihapus** dari analisis untuk memperbaiki performa model.
"""

# abs() --> mengembalikan nilai non-negatif dari suatu bilangan. Co = -0.7 --> 0.7
corr_matrix = new_data.corr().abs()
upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))
to_drop_hicorr = [column for column in upper.columns if any(upper[column] > 0.7)]

"""Pemilihan segitiga atas dari matriks korelasi biasanya dilakukan karena elemen-elemen di bawah diagonal utama adalah mirror image dari elemen-elemen di atas diagonal utama. Dengan kata lain, korelasi antara variabel A dan B adalah sama dengan korelasi antara variabel B dan A. Oleh karena itu, kita hanya perlu memeriksa setengah matriks korelasi untuk menghindari duplikasi dan menghemat waktu komputasi."""

# Visualisasi heatmap
plt.figure(figsize=(15, 10))
sns.heatmap(upper, annot=True, fmt='.1f')

to_drop_hicorr

new_data.drop(to_drop_hicorr, axis=1, inplace=True)

"""### Check Categorical Features"""

# nunique() --> menghitung jumlah nilai unik (distinct) dalam suatu Seri atau DataFrame
new_data.select_dtypes(include='object').nunique()

# Menghapus fitur yang memiliki nilai unik tinggi dan hanya memiliki satu nilai unik
new_data.drop(['emp_title', 'title', 'application_type'], axis=1, inplace=True)

new_data.select_dtypes(exclude='object').nunique()

# Menghapus fitur yang hanya memiliki satu nilai unik
new_data.drop(['policy_code'], axis=1, inplace=True)

for col in new_data.select_dtypes(include='object').columns.tolist():
  print(new_data[col].value_counts(normalize=True)*100)
  print('\n')

# Menghapus fitur yang didominasi oleh satu nilai saja
new_data.drop('pymnt_plan', axis=1, inplace=True)

"""##**MISSING VALUES**

### Missing Value Checking
"""

check_missing = new_data.isnull().sum()*100 / new_data.shape[0]
check_missing[check_missing > 0].sort_values(ascending=False)

# Menghapus kolom dengan miss value diatas 75%
new_data.drop('mths_since_last_record', axis=1, inplace=True)

"""### Missing Value Filling"""

new_data['annual_inc'].fillna(data['annual_inc'].mean(), inplace=True)
new_data['mths_since_earliest_cr_line'].fillna(0, inplace=True)
new_data['acc_now_delinq'].fillna(0, inplace=True)
new_data['total_acc'].fillna(0, inplace=True)
new_data['pub_rec'].fillna(0, inplace=True)
new_data['open_acc'].fillna(0, inplace=True)
new_data['inq_last_6mths'].fillna(0, inplace=True)
new_data['delinq_2yrs'].fillna(0, inplace=True)
new_data['collections_12_mths_ex_med'].fillna(0, inplace=True)
new_data['revol_util'].fillna(0, inplace=True)
new_data['emp_length_int'].fillna(0, inplace=True)
new_data['tot_cur_bal'].fillna(0, inplace=True)
new_data['tot_coll_amt'].fillna(0, inplace=True)
new_data['mths_since_last_delinq'].fillna(-1, inplace=True)

"""## **FEATURE SCALING AND TRANSFORMATION**

### One Hot Encoding
"""

categorical_cols = [col for col in new_data.select_dtypes(include='object').columns.tolist()]

# drop_first=True --> Satu dari kategori dalam variabel kategorikal akan dijatuhkan (dropped), yang berarti satu kategori tidak akan diwakili oleh variabel dummy.
onehot = pd.get_dummies(new_data[categorical_cols], drop_first=True)

onehot.head()

"""### Standardization

Semua kolom numeric dilakukan proses standarisasi dengan StandardScaler

StandardScaler adalah proses normalisasi data numerik sehingga setiap fitur (kolom) memiliki rata-rata 0 dan variansi 1.

Dengan melakukan standarisasi, kita memastikan bahwa tidak ada satu fitur pun yang mendominasi proses pembelajaran model hanya karena skala yang besar, dan ini dapat membantu meningkatkan kinerja dan stabilitas model.
"""

numerical_cols = [col for col in new_data.columns.tolist() if col not in categorical_cols + ['status_loan']]

ss = StandardScaler()
std = pd.DataFrame(ss.fit_transform(new_data[numerical_cols]), columns=numerical_cols)

std.head()

"""### Join Transformed Dataframe"""

data_model = pd.concat([onehot, std, new_data[['status_loan']]], axis=1)

data_model.head()

"""##**MODELING**

### Train-Test Split
"""

X = data_model.drop('status_loan', axis=1)
y = data_model['status_loan']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

X_train.shape, X_test.shape

# Menghitung jumlah setiap nilai dalam y_train
value_counts_train = y_train.value_counts()

# Membuat grafik menggunakan Seaborn
plt.figure(figsize=(5, 4))
sns.barplot(x=value_counts_train.index, y=value_counts_train.values, palette='pastel')
plt.xlabel('Class')
plt.ylabel('Count')

# Menambahkan jumlah nilai setiap kelas pada grafik
for i in range(len(value_counts_train)):
    plt.text(i, value_counts_train[i], str(value_counts_train[i]), ha='center', va='bottom')

"""### Oversampling SMOTE Data Train"""

# Oversampling menggunakan SMOTE
smote = SMOTE(random_state=42)
X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)

# Menghitung jumlah setiap nilai dalam y_train_resampled
value_counts_train_res = y_train_resampled.value_counts()

# Membuat grafik menggunakan Seaborn
plt.figure(figsize=(5, 4))
sns.barplot(x=value_counts_train_res.index, y=value_counts_train_res.values, palette='pastel')
plt.xlabel('Class')
plt.ylabel('Count')

# Menambahkan jumlah nilai setiap kelas pada grafik
for i in range(len(value_counts_train_res)):
    plt.text(i, value_counts_train_res[i], str(value_counts_train_res[i]), ha='center', va='bottom')

"""### Random Forest

#### Training Random Forest
"""

rfc = RandomForestClassifier(max_depth=4)
rfc.fit(X_train, y_train)

arr_feature_importances_rfc = rfc.feature_importances_
arr_feature_names_rfc = X_train.columns.values

df_feature_importance_rfc = pd.DataFrame(index=range(len(arr_feature_importances_rfc)), columns=['feature', 'importance'])
df_feature_importance_rfc['feature'] = arr_feature_names_rfc
df_feature_importance_rfc['importance'] = arr_feature_importances_rfc
df_all_features_rfc = df_feature_importance_rfc.sort_values(by='importance', ascending=False)
df_all_features_rfc

"""#### Validation Random Forest

Metrik yang umum dipakai dalam dunia credit risk adalah AUC dan KS (kolmogorov-Smirnov)
"""

y_pred_proba_rfc = rfc.predict_proba(X_test)[:][:,1]

df_actual_predicted_rfc = pd.concat([pd.DataFrame(np.array(y_test), columns=['y_actual']), pd.DataFrame(y_pred_proba_rfc, columns=['y_pred_proba'])], axis=1)
df_actual_predicted_rfc.index = y_test.index

"""##### AUC"""

fpr, tpr, tr = roc_curve(df_actual_predicted_rfc['y_actual'], df_actual_predicted_rfc['y_pred_proba'])
auc_rfc = roc_auc_score(df_actual_predicted_rfc['y_actual'], df_actual_predicted_rfc['y_pred_proba'])

plt.plot(fpr, tpr, label='AUC = %0.4f' %auc_rfc)
plt.plot(fpr, fpr, linestyle = '--', color='k')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve')
plt.legend()

"""##### KS"""

df_actual_predicted_rfc = df_actual_predicted_rfc.sort_values('y_pred_proba')
df_actual_predicted_rfc = df_actual_predicted_rfc.reset_index()

df_actual_predicted_rfc['Cumulative N Population'] = df_actual_predicted_rfc.index + 1
df_actual_predicted_rfc['Cumulative N Bad'] = df_actual_predicted_rfc['y_actual'].cumsum()
df_actual_predicted_rfc['Cumulative N Good'] = df_actual_predicted_rfc['Cumulative N Population'] - df_actual_predicted_rfc['Cumulative N Bad']
df_actual_predicted_rfc['Cumulative Perc Population'] = df_actual_predicted_rfc['Cumulative N Population'] / df_actual_predicted_rfc.shape[0]
df_actual_predicted_rfc['Cumulative Perc Bad'] = df_actual_predicted_rfc['Cumulative N Bad'] / df_actual_predicted_rfc['y_actual'].sum()
df_actual_predicted_rfc['Cumulative Perc Good'] = df_actual_predicted_rfc['Cumulative N Good'] / (df_actual_predicted_rfc.shape[0] - df_actual_predicted_rfc['y_actual'].sum())

df_actual_predicted_rfc.head()

KS_rfc = max(df_actual_predicted_rfc['Cumulative Perc Good'] - df_actual_predicted_rfc['Cumulative Perc Bad'])

plt.plot(df_actual_predicted_rfc['y_pred_proba'], df_actual_predicted_rfc['Cumulative Perc Bad'], color='r')
plt.plot(df_actual_predicted_rfc['y_pred_proba'], df_actual_predicted_rfc['Cumulative Perc Good'], color='b')
plt.xlabel('Estimated Probability for Being Bad')
plt.ylabel('Cumulative %')
plt.title('Kolmogorov-Smirnov:  %0.4f' %KS_rfc)

"""### Boosting

#### Training Gboost
"""

# Inisialisasi model Gradient Boosting Classifier
gb_clf = GradientBoostingClassifier()

# Latih model pada data latih
gb_clf.fit(X_train, y_train)

# Mendapatkan nilai penting fitur
arr_feature_importances_gb = gb_clf.feature_importances_
arr_feature_names_gb = X_train.columns.values

# Membuat DataFrame untuk menyimpan nilai penting fitur
df_feature_importance_gb = pd.DataFrame(index=range(len(arr_feature_importances_gb)), columns=['feature', 'importance'])
df_feature_importance_gb['feature'] = arr_feature_names_gb
df_feature_importance_gb['importance'] = arr_feature_importances_gb
df_all_features_gb = df_feature_importance_gb.sort_values(by='importance', ascending=False)
print(df_all_features_gb)

"""#### Validation Gboost"""

y_pred_proba_gb = gb_clf.predict_proba(X_test)[:][:,1]

df_actual_predicted_gb = pd.concat([pd.DataFrame(np.array(y_test), columns=['y_actual']), pd.DataFrame(y_pred_proba_gb, columns=['y_pred_proba'])], axis=1)
df_actual_predicted_gb.index = y_test.index

"""##### AUC"""

fpr, tpr, tr = roc_curve(df_actual_predicted_gb['y_actual'], df_actual_predicted_gb['y_pred_proba'])
auc_gb = roc_auc_score(df_actual_predicted_gb['y_actual'], df_actual_predicted_gb['y_pred_proba'])

plt.plot(fpr, tpr, label='AUC = %0.4f' %auc_gb)
plt.plot(fpr, fpr, linestyle = '--', color='k')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve')
plt.legend()

"""##### KS"""

df_actual_predicted_gb = df_actual_predicted_gb.sort_values('y_pred_proba')
df_actual_predicted_gb = df_actual_predicted_gb.reset_index()

df_actual_predicted_gb['Cumulative N Population'] = df_actual_predicted_gb.index + 1
df_actual_predicted_gb['Cumulative N Bad'] = df_actual_predicted_gb['y_actual'].cumsum()
df_actual_predicted_gb['Cumulative N Good'] = df_actual_predicted_gb['Cumulative N Population'] - df_actual_predicted_gb['Cumulative N Bad']
df_actual_predicted_gb['Cumulative Perc Population'] = df_actual_predicted_gb['Cumulative N Population'] / df_actual_predicted_gb.shape[0]
df_actual_predicted_gb['Cumulative Perc Bad'] = df_actual_predicted_gb['Cumulative N Bad'] / df_actual_predicted_gb['y_actual'].sum()
df_actual_predicted_gb['Cumulative Perc Good'] = df_actual_predicted_gb['Cumulative N Good'] / (df_actual_predicted_gb.shape[0] - df_actual_predicted_gb['y_actual'].sum())

df_actual_predicted_gb.head()

KS_gb = max(df_actual_predicted_gb['Cumulative Perc Good'] - df_actual_predicted_gb['Cumulative Perc Bad'])

plt.plot(df_actual_predicted_gb['y_pred_proba'], df_actual_predicted_gb['Cumulative Perc Bad'], color='r')
plt.plot(df_actual_predicted_gb['y_pred_proba'], df_actual_predicted_gb['Cumulative Perc Good'], color='b')
plt.xlabel('Estimated Probability for Being Bad')
plt.ylabel('Cumulative %')
plt.title('Kolmogorov-Smirnov:  %0.4f' %KS_gb)

"""#### Overfitting Check"""

# # Evaluasi performa model pada data uji
# accuracy_test = gb_clf.score(X_test, y_test)
# print("Akurasi pada data uji:", accuracy_test)

# # Perbandingan performa pada data latih dan data uji
# accuracy_train = gb_clf.score(X_train, y_train)
# print("Akurasi pada data latih:", accuracy_train)

# # Analisis kurva pembelajaran
# from sklearn.model_selection import learning_curve

# train_sizes, train_scores, test_scores = learning_curve(gb_clf, X_train, y_train, cv=5)

# plt.figure(figsize=(10, 6))
# plt.plot(train_sizes, np.mean(train_scores, axis=1), label='Train')
# plt.plot(train_sizes, np.mean(test_scores, axis=1), label='Test')
# plt.xlabel('Training examples')
# plt.ylabel('Score')
# plt.title('Learning Curve')
# plt.legend()
# plt.show()

# # Analisis kurva validasi silang
# from sklearn.model_selection import validation_curve

# param_range = [3, 4, 5, 6, 7] # Contoh range untuk hyperparameter yang ingin diselidiki, misalnya max_depth

# train_scores, test_scores = validation_curve(
#     gb_clf, X_train, y_train, param_name="max_depth", param_range=param_range,
#     cv=5, scoring="accuracy", n_jobs=1)

# plt.figure(figsize=(10, 6))
# plt.plot(param_range, np.mean(train_scores, axis=1), label='Train')
# plt.plot(param_range, np.mean(test_scores, axis=1), label='Test')
# plt.xlabel('Max Depth')
# plt.ylabel('Score')
# plt.title('Validation Curve')
# plt.legend()
# plt.show()

"""### Logistic Regression

#### Training LR
"""

# Inisialisasi model Logistic Regression
lr_model = LogisticRegression(random_state=42)

# Melatih model pada data latih
lr_model.fit(X_train, y_train)

"""#### Validation LR"""

y_pred_proba_lr = lr_model.predict_proba(X_test)[:][:,1]

df_actual_predicted_lr = pd.concat([pd.DataFrame(np.array(y_test), columns=['y_actual']), pd.DataFrame(y_pred_proba_lr, columns=['y_pred_proba'])], axis=1)
df_actual_predicted_lr.index = y_test.index

"""##### AUC"""

fpr, tpr, tr = roc_curve(df_actual_predicted_lr['y_actual'], df_actual_predicted_lr['y_pred_proba'])
auc_lr = roc_auc_score(df_actual_predicted_lr['y_actual'], df_actual_predicted_lr['y_pred_proba'])

plt.plot(fpr, tpr, label='AUC = %0.4f' %auc_lr)
plt.plot(fpr, fpr, linestyle = '--', color='k')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve')
plt.legend()

"""##### KS"""

df_actual_predicted_lr = df_actual_predicted_lr.sort_values('y_pred_proba')
df_actual_predicted_lr = df_actual_predicted_lr.reset_index()

df_actual_predicted_lr['Cumulative N Population'] = df_actual_predicted_lr.index + 1
df_actual_predicted_lr['Cumulative N Bad'] = df_actual_predicted_lr['y_actual'].cumsum()
df_actual_predicted_lr['Cumulative N Good'] = df_actual_predicted_lr['Cumulative N Population'] - df_actual_predicted_lr['Cumulative N Bad']
df_actual_predicted_lr['Cumulative Perc Population'] = df_actual_predicted_lr['Cumulative N Population'] / df_actual_predicted_lr.shape[0]
df_actual_predicted_lr['Cumulative Perc Bad'] = df_actual_predicted_lr['Cumulative N Bad'] / df_actual_predicted_lr['y_actual'].sum()
df_actual_predicted_lr['Cumulative Perc Good'] = df_actual_predicted_lr['Cumulative N Good'] / (df_actual_predicted_lr.shape[0] - df_actual_predicted_lr['y_actual'].sum())

df_actual_predicted_lr.head()

KS_lr = max(df_actual_predicted_lr['Cumulative Perc Good'] - df_actual_predicted_lr['Cumulative Perc Bad'])

plt.plot(df_actual_predicted_lr['y_pred_proba'], df_actual_predicted_lr['Cumulative Perc Bad'], color='r')
plt.plot(df_actual_predicted_lr['y_pred_proba'], df_actual_predicted_lr['Cumulative Perc Good'], color='b')
plt.xlabel('Estimated Probability for Being Bad')
plt.ylabel('Cumulative %')
plt.title('Kolmogorov-Smirnov:  %0.4f' %KS_lr)

"""##**KESIMPULAN**

**Random Forest**


*   AUC = 0.859
*   KS = 0.56

**GBoost**


*   AUC = 0.879
*   KS = 0.61

**Logistic Regression**


*   AUC = 0.849
*   KS = 0.56

Dari 3 algoritma tersebut, algoritma Gboost memiliki nilai AUC dan KS lebih tinggi daripada RF dan LR dengan `AUC 0.879` dan `KS 0.61`

Pada dunia credit risk modeling, umumnya AUC di atas 0.7 dan KS di atas 0.3 sudah termasuk performa yang baik.
"""